{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# COVID 19 Severity Prediction  \n",
    "**Goal**: For a given individual who is diagnosed with COVID 19 we want to determine the severity of their infection in terms of the probability of decease and probability of ICU admittance.  \n",
    "**Dataset**: The dataset available to us is curated by the Mexico Government and contains information for 500k+ observations. It contains information regarding the result of the test, time of detection, demographic information and most importantly that of various health conditions.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(566602, 23)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>sex</th>\n",
       "      <th>patient_type</th>\n",
       "      <th>entry_date</th>\n",
       "      <th>date_symptoms</th>\n",
       "      <th>date_died</th>\n",
       "      <th>intubed</th>\n",
       "      <th>pneumonia</th>\n",
       "      <th>age</th>\n",
       "      <th>pregnancy</th>\n",
       "      <th>...</th>\n",
       "      <th>inmsupr</th>\n",
       "      <th>hypertension</th>\n",
       "      <th>other_disease</th>\n",
       "      <th>cardiovascular</th>\n",
       "      <th>obesity</th>\n",
       "      <th>renal_chronic</th>\n",
       "      <th>tobacco</th>\n",
       "      <th>contact_other_covid</th>\n",
       "      <th>covid_res</th>\n",
       "      <th>icu</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>16169f</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>04-05-2020</td>\n",
       "      <td>02-05-2020</td>\n",
       "      <td>9999-99-99</td>\n",
       "      <td>97</td>\n",
       "      <td>2</td>\n",
       "      <td>27</td>\n",
       "      <td>97</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>97</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1009bf</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>19-03-2020</td>\n",
       "      <td>17-03-2020</td>\n",
       "      <td>9999-99-99</td>\n",
       "      <td>97</td>\n",
       "      <td>2</td>\n",
       "      <td>24</td>\n",
       "      <td>97</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>99</td>\n",
       "      <td>1</td>\n",
       "      <td>97</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>167386</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>06-04-2020</td>\n",
       "      <td>01-04-2020</td>\n",
       "      <td>9999-99-99</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>54</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>99</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0b5948</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>17-04-2020</td>\n",
       "      <td>10-04-2020</td>\n",
       "      <td>9999-99-99</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>30</td>\n",
       "      <td>97</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>99</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0d01b5</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>13-04-2020</td>\n",
       "      <td>13-04-2020</td>\n",
       "      <td>22-04-2020</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>60</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>99</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 23 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       id  sex  patient_type  entry_date date_symptoms   date_died  intubed  \\\n",
       "0  16169f    2             1  04-05-2020    02-05-2020  9999-99-99       97   \n",
       "1  1009bf    2             1  19-03-2020    17-03-2020  9999-99-99       97   \n",
       "2  167386    1             2  06-04-2020    01-04-2020  9999-99-99        2   \n",
       "3  0b5948    2             2  17-04-2020    10-04-2020  9999-99-99        2   \n",
       "4  0d01b5    1             2  13-04-2020    13-04-2020  22-04-2020        2   \n",
       "\n",
       "   pneumonia  age  pregnancy  ...  inmsupr  hypertension  other_disease  \\\n",
       "0          2   27         97  ...        2             2              2   \n",
       "1          2   24         97  ...        2             2              2   \n",
       "2          2   54          2  ...        2             2              2   \n",
       "3          1   30         97  ...        2             2              2   \n",
       "4          2   60          2  ...        2             1              2   \n",
       "\n",
       "   cardiovascular  obesity  renal_chronic  tobacco  contact_other_covid  \\\n",
       "0               2        2              2        2                    2   \n",
       "1               2        2              2        2                   99   \n",
       "2               2        1              2        2                   99   \n",
       "3               2        2              2        2                   99   \n",
       "4               1        2              2        2                   99   \n",
       "\n",
       "   covid_res  icu  \n",
       "0          1   97  \n",
       "1          1   97  \n",
       "2          1    2  \n",
       "3          1    2  \n",
       "4          1    2  \n",
       "\n",
       "[5 rows x 23 columns]"
      ]
     },
     "execution_count": 260,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import and asses dataset\n",
    "# Dataset description can be found at: https://www.kaggle.com/tanmoyx/covid19-patient-precondition-dataset?select=covid.csv\n",
    "df = pd.read_csv(\"../data/mexico_government_covid19_patient/covid.csv\")\n",
    "print(df.shape)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the date_died is invalid then the given case of COVID 19 did not result in fatality. So we form a new feature \"lethal\" indicating this. All invalid dates are replaced with no(2), and all valid dates are replaced with 1(yes)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>sex</th>\n",
       "      <th>patient_type</th>\n",
       "      <th>entry_date</th>\n",
       "      <th>date_symptoms</th>\n",
       "      <th>lethal</th>\n",
       "      <th>intubed</th>\n",
       "      <th>pneumonia</th>\n",
       "      <th>age</th>\n",
       "      <th>pregnancy</th>\n",
       "      <th>...</th>\n",
       "      <th>inmsupr</th>\n",
       "      <th>hypertension</th>\n",
       "      <th>other_disease</th>\n",
       "      <th>cardiovascular</th>\n",
       "      <th>obesity</th>\n",
       "      <th>renal_chronic</th>\n",
       "      <th>tobacco</th>\n",
       "      <th>contact_other_covid</th>\n",
       "      <th>covid_res</th>\n",
       "      <th>icu</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>16169f</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>04-05-2020</td>\n",
       "      <td>02-05-2020</td>\n",
       "      <td>2</td>\n",
       "      <td>97</td>\n",
       "      <td>2</td>\n",
       "      <td>27</td>\n",
       "      <td>97</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>97</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1009bf</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>19-03-2020</td>\n",
       "      <td>17-03-2020</td>\n",
       "      <td>2</td>\n",
       "      <td>97</td>\n",
       "      <td>2</td>\n",
       "      <td>24</td>\n",
       "      <td>97</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>99</td>\n",
       "      <td>1</td>\n",
       "      <td>97</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>167386</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>06-04-2020</td>\n",
       "      <td>01-04-2020</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>54</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>99</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0b5948</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>17-04-2020</td>\n",
       "      <td>10-04-2020</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>30</td>\n",
       "      <td>97</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>99</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0d01b5</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>13-04-2020</td>\n",
       "      <td>13-04-2020</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>60</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>99</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 23 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       id  sex  patient_type  entry_date date_symptoms lethal  intubed  \\\n",
       "0  16169f    2             1  04-05-2020    02-05-2020      2       97   \n",
       "1  1009bf    2             1  19-03-2020    17-03-2020      2       97   \n",
       "2  167386    1             2  06-04-2020    01-04-2020      2        2   \n",
       "3  0b5948    2             2  17-04-2020    10-04-2020      2        2   \n",
       "4  0d01b5    1             2  13-04-2020    13-04-2020      1        2   \n",
       "\n",
       "   pneumonia  age  pregnancy  ...  inmsupr  hypertension  other_disease  \\\n",
       "0          2   27         97  ...        2             2              2   \n",
       "1          2   24         97  ...        2             2              2   \n",
       "2          2   54          2  ...        2             2              2   \n",
       "3          1   30         97  ...        2             2              2   \n",
       "4          2   60          2  ...        2             1              2   \n",
       "\n",
       "   cardiovascular  obesity  renal_chronic  tobacco  contact_other_covid  \\\n",
       "0               2        2              2        2                    2   \n",
       "1               2        2              2        2                   99   \n",
       "2               2        1              2        2                   99   \n",
       "3               2        2              2        2                   99   \n",
       "4               1        2              2        2                   99   \n",
       "\n",
       "   covid_res  icu  \n",
       "0          1   97  \n",
       "1          1   97  \n",
       "2          1    2  \n",
       "3          1    2  \n",
       "4          1    2  \n",
       "\n",
       "[5 rows x 23 columns]"
      ]
     },
     "execution_count": 261,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Change the data died column to indicate if patient death\n",
    "# If there is a valid date then patient death and represent as 1 otherwise represent as 2\n",
    "df['date_died'] = df['date_died'].replace(to_replace=\"9999-99-99\", value=2)\n",
    "df['date_died'] = df['date_died'].mask(df['date_died'].ne(2), 1)\n",
    "df = df.rename(columns={'date_died':'lethal'})\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The features entry_date and date_symptoms provide valuable information, but by themselves they are not relevant to determining the severity of COVID 19. A more relevant feature would be the time between entry_date and data_symptoms. So we create a feature by subtracting these two values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>sex</th>\n",
       "      <th>patient_type</th>\n",
       "      <th>lethal</th>\n",
       "      <th>intubed</th>\n",
       "      <th>pneumonia</th>\n",
       "      <th>age</th>\n",
       "      <th>pregnancy</th>\n",
       "      <th>diabetes</th>\n",
       "      <th>copd</th>\n",
       "      <th>...</th>\n",
       "      <th>hypertension</th>\n",
       "      <th>other_disease</th>\n",
       "      <th>cardiovascular</th>\n",
       "      <th>obesity</th>\n",
       "      <th>renal_chronic</th>\n",
       "      <th>tobacco</th>\n",
       "      <th>contact_other_covid</th>\n",
       "      <th>covid_res</th>\n",
       "      <th>icu</th>\n",
       "      <th>days_to_medical_help</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>16169f</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>97</td>\n",
       "      <td>2</td>\n",
       "      <td>27</td>\n",
       "      <td>97</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>97</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1009bf</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>97</td>\n",
       "      <td>2</td>\n",
       "      <td>24</td>\n",
       "      <td>97</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>99</td>\n",
       "      <td>1</td>\n",
       "      <td>97</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>167386</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>54</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>99</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0b5948</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>30</td>\n",
       "      <td>97</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>99</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0d01b5</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>60</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>99</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 22 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       id  sex  patient_type lethal  intubed  pneumonia  age  pregnancy  \\\n",
       "0  16169f    2             1      2       97          2   27         97   \n",
       "1  1009bf    2             1      2       97          2   24         97   \n",
       "2  167386    1             2      2        2          2   54          2   \n",
       "3  0b5948    2             2      2        2          1   30         97   \n",
       "4  0d01b5    1             2      1        2          2   60          2   \n",
       "\n",
       "   diabetes  copd  ...  hypertension  other_disease  cardiovascular  obesity  \\\n",
       "0         2     2  ...             2              2               2        2   \n",
       "1         2     2  ...             2              2               2        2   \n",
       "2         2     2  ...             2              2               2        1   \n",
       "3         2     2  ...             2              2               2        2   \n",
       "4         1     2  ...             1              2               1        2   \n",
       "\n",
       "   renal_chronic  tobacco  contact_other_covid  covid_res  icu  \\\n",
       "0              2        2                    2          1   97   \n",
       "1              2        2                   99          1   97   \n",
       "2              2        2                   99          1    2   \n",
       "3              2        2                   99          1    2   \n",
       "4              2        2                   99          1    2   \n",
       "\n",
       "   days_to_medical_help  \n",
       "0                     2  \n",
       "1                     2  \n",
       "2                     5  \n",
       "3                     7  \n",
       "4                     0  \n",
       "\n",
       "[5 rows x 22 columns]"
      ]
     },
     "execution_count": 262,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# With date_symptoms and entry_date we can calculate time between disease contraction and medical attention\n",
    "# This would be a more relevant feature to use rather than just the dates\n",
    "df['days_to_medical_help'] = (pd.to_datetime(df['entry_date'], dayfirst=True) - pd.to_datetime(df['date_symptoms'], dayfirst=True)).dt.days\n",
    "df = df.drop(columns=['entry_date', 'date_symptoms'])\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop Patient ID from the dataset since it won't help in predictions\n",
    "columns_to_drop = ['id']\n",
    "df = df.drop(columns=columns_to_drop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sex                      int64\n",
       "patient_type             int64\n",
       "lethal                  object\n",
       "intubed                  int64\n",
       "pneumonia                int64\n",
       "age                      int64\n",
       "pregnancy                int64\n",
       "diabetes                 int64\n",
       "copd                     int64\n",
       "asthma                   int64\n",
       "inmsupr                  int64\n",
       "hypertension             int64\n",
       "other_disease            int64\n",
       "cardiovascular           int64\n",
       "obesity                  int64\n",
       "renal_chronic            int64\n",
       "tobacco                  int64\n",
       "contact_other_covid      int64\n",
       "covid_res                int64\n",
       "icu                      int64\n",
       "days_to_medical_help     int64\n",
       "dtype: object"
      ]
     },
     "execution_count": 264,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check data types of all features\n",
    "df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now even though we know that most of these variables are categorical pandas has encoded them as int or floats so we must convert them to categorical. We also would want to normalize all non-categorical variables and replace missing values encoded as 97, 98, and 99 in the categorical variable. We also change the encoding from 2 -> No to 0 -> to No."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Since most categorical features are currently int64 change them to categorical\n",
    "# We also want to normalize all non categorical features\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from joblib import dump, load\n",
    "categorical_columns = ['sex', 'patient_type',\n",
    "       'intubed', 'pneumonia', 'pregnancy', 'diabetes', 'copd',\n",
    "       'asthma', 'inmsupr', 'hypertension', 'other_disease', 'cardiovascular',\n",
    "       'obesity', 'renal_chronic', 'tobacco', 'contact_other_covid',\n",
    "       'covid_res', 'icu', 'lethal']\n",
    "# The dataset indicates that missing values are 97, 98, 99 so we replace them with -1 for uniformity\n",
    "scalers = {}\n",
    "for column in df.columns:\n",
    "    if column in categorical_columns:\n",
    "        # Change no encoding from 2 to 0\n",
    "        df[column] = df[column].replace(to_replace=2, value=0)\n",
    "        # Replace all missing values with -1\n",
    "        df[column] = df[column].replace(to_replace=[97, 98, 99], value=-1).astype('category')\n",
    "    else:\n",
    "        # Normalize non categorical features\n",
    "        scalers[column] = MinMaxScaler().fit(np.array(df[column]).reshape(-1, 1))\n",
    "        df[column] = scalers[column].transform(np.array(df[column]).reshape(-1, 1))[:, 0]\n",
    "        dump(scalers[column], str(column) + '_scaler.joblib') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(220657, 21)"
      ]
     },
     "execution_count": 266,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Now we can only use observations in which the individual tested postive for COVID 19\n",
    "covid_cases = df[df['covid_res'] == 1]\n",
    "covid_cases.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(66910, 21)"
      ]
     },
     "execution_count": 267,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Encoding of 3 for 'covid_res' is supposed to be awaiting results so we can use these as the prediction set for risk assesment\n",
    "awaiting_cases = df[df['covid_res'] == 3]\n",
    "awaiting_cases.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Variables to store the outcomes and features\n",
    "# We could try combining icu and death\n",
    "# However we do not want to include either in the prediction for the other due to expected high dependency\n",
    "\"\"\"features = ['sex', 'patient_type', 'age', 'days_to_medical_help',\n",
    "       'intubed', 'pneumonia', 'pregnancy', 'diabetes', 'copd',\n",
    "       'asthma', 'inmsupr', 'hypertension', 'other_disease', 'cardiovascular',\n",
    "       'obesity', 'renal_chronic', 'tobacco']\"\"\"\n",
    "features = ['sex', 'age', 'days_to_medical_help', 'intubed', 'pneumonia', 'pregnancy', 'diabetes', 'copd',\n",
    "       'asthma', 'inmsupr', 'hypertension', 'other_disease', 'cardiovascular',\n",
    "       'obesity', 'renal_chronic', 'tobacco']\n",
    "outcomes = ['icu', 'death']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ICU Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(68210, 21)"
      ]
     },
     "execution_count": 269,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# First we train a model to predict 'icu' for that we form a dataset of known icu cases i.e Yes or No so we remove 0s which were\n",
    "# supposed to be missing values or cases in which we do not know if the individual was in the icu\n",
    "current_outcome = outcomes[0] # i.e 'icu'\n",
    "icu_res_known = covid_cases[covid_cases['icu'] != -1]\n",
    "icu_res_known.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data into train and test\n",
    "from sklearn.model_selection import train_test_split\n",
    "train, test = train_test_split(icu_res_known, shuffle=True, test_size=0.3, stratify=icu_res_known['icu'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:xlabel='icu', ylabel='count'>"
      ]
     },
     "execution_count": 271,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Before we train we want to check the distribution of icu in the train set to ensure proper training\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "sns.countplot(x='icu', data=train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The above graph indicates that the data is highly biased so we must perform some sampling to balance the dataset\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from imblearn.pipeline import Pipeline\n",
    "# Over Sample the Minority Label to be 0.3 in proportion\n",
    "over = SMOTE(sampling_strategy=0.3)\n",
    "# Down Sample the Majority Label so that there are twice as many as minority\n",
    "under = RandomUnderSampler(sampling_strategy=0.5)\n",
    "# Pipeline to combine \n",
    "steps = [('o', over), ('u', under)]\n",
    "pipeline = Pipeline(steps=steps)\n",
    "# Sampled data\n",
    "obs, results = pipeline.fit_resample(train[features], train['icu'])\n",
    "obs = obs.fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(26202, 13101)"
      ]
     },
     "execution_count": 273,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check if sampling has given the desired results\n",
    "len(results[results == 0]), len(results[results == 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only down sample the \"no\" icu observations\n",
    "icu_yes = train[train['icu'] == 1]\n",
    "icu_no = train[train['icu'] == 0]\n",
    "new_train = icu_yes.append(icu_no.sample(icu_yes.shape[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**SMOTE Random Forest**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8809558715730831\n",
      "Precision: [0.9441292  0.33864169]\n",
      "Recall: [0.92455653 0.41385232]\n",
      "F-Score: [0.93424036 0.37248841]\n"
     ]
    }
   ],
   "source": [
    "# Train Random Forest as a baseline model\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "rf = RandomForestClassifier(n_estimators=20).fit(obs, results)\n",
    "print(\"Accuracy: {}\".format(rf.score(test[features], test['icu'])))\n",
    "metrics = precision_recall_fscore_support(test['icu'], rf.predict(test[features]))\n",
    "print(\"Precision: {}\\nRecall: {}\\nF-Score: {}\".format(metrics[0], metrics[1], metrics[2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.73019596344622\n",
      "Precision: [0.96107345 0.19337017]\n",
      "Recall: [0.73477239 0.68116772]\n",
      "F-Score: [0.83282362 0.30122769]\n"
     ]
    }
   ],
   "source": [
    "# Train Random Forest as a baseline model\n",
    "down_rf = RandomForestClassifier(n_estimators=100).fit(new_train[features], new_train['icu'])\n",
    "print(\"Accuracy: {}\".format(down_rf.score(test[features], test['icu'])))\n",
    "metrics = precision_recall_fscore_support(test['icu'], down_rf.predict(test[features]))\n",
    "print(\"Precision: {}\\nRecall: {}\\nF-Score: {}\".format(metrics[0], metrics[1], metrics[2]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Normal Data Random Forest**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9179006010848849\n",
      "Precision: [0.92985466 0.55177743]\n",
      "Recall: [0.98450524 0.20435031]\n",
      "F-Score: [0.95639988 0.29824561]\n"
     ]
    }
   ],
   "source": [
    "# Let us test for the non sampled dataset\n",
    "rf_normal = RandomForestClassifier(n_estimators=10, max_depth=10).fit(train[features], train['icu'])\n",
    "print(\"Accuracy: {}\".format(rf_normal.score(test[features], test['icu'])))\n",
    "metrics_normal = precision_recall_fscore_support(test['icu'], rf_normal.predict(test[features]))\n",
    "print(\"Precision: {}\\nRecall: {}\\nF-Score: {}\".format(metrics_normal[0], metrics_normal[1], metrics_normal[2]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**SMOTE & Training Functions**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets repeat the procedure for \"death\" risk prediciton but with a function since that might help with the API implementation\n",
    "def checkBinaryBalance(data, features, output, negative_label=0, positive_label=1):\n",
    "    data_arr = np.array(data[output])\n",
    "    if abs(data_arr[data_arr == negative_label]. shape[0] / len(data) - data_arr[data_arr == positive_label].shape[0] / len(data_arr)) > 0.4:\n",
    "        print(\"Not Balanced returning sampled dataet.\")\n",
    "        # Over Sample the Minority Label to be 0.3 in proportion\n",
    "        over = SMOTE(sampling_strategy=0.3)\n",
    "        # Down Sample the Majority Label so that there are twice as many as minority\n",
    "        under = RandomUnderSampler(sampling_strategy=0.5)\n",
    "        # Pipeline to combine \n",
    "        steps = [('o', over), ('u', under)]\n",
    "        pipeline = Pipeline(steps=steps)\n",
    "        # Sampled data\n",
    "        obs, results = pipeline.fit_resample(data[features], data_arr)\n",
    "        obs = obs.fillna(0)\n",
    "        return obs, results\n",
    "    print(\"Balanced dataset returning original.\")\n",
    "    obs = data[features]\n",
    "    results = data_arr\n",
    "    return obs, results\n",
    "\n",
    "def randomForestEvaluation(obs, results, test_obs, test_results):\n",
    "    rf = RandomForestClassifier(n_estimators=40, max_depth=10).fit(obs, results)\n",
    "    print(\"Accuracy: {}\".format(rf.score(test_obs, test_results)))\n",
    "    metrics = precision_recall_fscore_support(test_results, rf.predict(test_obs))\n",
    "    print(\"Precision: {}\\nRecall: {}\\nF-Score: {}\".format(metrics[0], metrics[1], metrics[2]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**All sklearn-models**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now that we have trained on Random Forest Let us test the baselines for all classifiers within the sklearn library\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.gaussian_process import GaussianProcessClassifier\n",
    "from sklearn.gaussian_process.kernels import RBF\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
    "\n",
    "classifiers = [\n",
    "    #KNeighborsClassifier(2),\n",
    "    #SVC(kernel=\"linear\", C=0.025),\n",
    "    #SVC(gamma=2, C=1),\n",
    "    #GaussianProcessClassifier(1.0 * RBF(1.0)),\n",
    "    DecisionTreeClassifier(),\n",
    "    RandomForestClassifier(n_estimators=20),\n",
    "    MLPClassifier(max_iter=10000),\n",
    "    AdaBoostClassifier(),\n",
    "    GaussianNB(),\n",
    "    #QuadraticDiscriminantAnalysis(),\n",
    "]\n",
    "\n",
    "\"\"\"names = [\"Nearest Neighbors\", \"Linear SVM\", \"RBF SVM\", \"Gaussian Process\",\n",
    "         \"Decision Tree\", \"Random Forest\", \"Neural Net\", \"AdaBoost\",\n",
    "         \"Naive Bayes\", \"QDA\"]\"\"\"\n",
    "names = [\"Decision Tree\", \"Random Forest\", \"Neural Net\", \"AdaBoost\",\n",
    "         \"Naive Bayes\"]\n",
    "\n",
    "def allModelEvaluation(names, classifiers, obs, results, test_obs, test_results):\n",
    "    scores = {}\n",
    "    for name, clf in zip(names, classifiers):\n",
    "        print(\"Training \" + name)\n",
    "        clf.fit(obs, results)\n",
    "        train_score =  clf.score(obs, results)\n",
    "        metrics_train = precision_recall_fscore_support(results, clf.predict(obs))\n",
    "        test_score = clf.score(test_obs, test_results)\n",
    "        metrics_test = precision_recall_fscore_support(test_results, clf.predict(test_obs))\n",
    "        print(\"Train Accuracy: {}\".format(train_score))\n",
    "        print(\"Train Precision: {}\\nTrain Recall: {}\\nTrain F-Score: {}\\n\".format(metrics_train[0], metrics_train[1], metrics_train[2]))\n",
    "        print(\"Test Accuracy: {}\".format(test_score))\n",
    "        print(\"Test Precision: {}\\nTest Recall: {}\\nTest F-Score: {}\".format(metrics_test[0], metrics_test[1], metrics_test[2]))\n",
    "        scores[name] = {'test accuracy' : test_score, 'test precision' : metrics_test[0], 'test recall': metrics_test[1], 'train accuracy' : train_score, 'train precision' : metrics_train[0], 'train recall' : metrics_train[1]}\n",
    "        print(\"-----------\")\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Not Balanced returning sampled dataet.\n",
      "Training Decision Tree\n",
      "Train Accuracy: 0.9577914110429447\n",
      "Train Precision: [0.93172877 0.98720293]\n",
      "Train Recall: [0.98797546 0.92760736]\n",
      "Train F-Score: [0.95902811 0.95647773]\n",
      "\n",
      "Test Accuracy: 0.7121145482089625\n",
      "Test Precision: [0.97893793 0.20709641]\n",
      "Test Recall: [0.7003099  0.83858042]\n",
      "Test F-Score: [0.81650833 0.33216189]\n",
      "-----------\n",
      "Training Random Forest\n",
      "Train Accuracy: 0.9505521472392638\n",
      "Train Precision: [0.94390716 0.9573991 ]\n",
      "Train Recall: [0.95803681 0.94306748]\n",
      "Train F-Score: [0.9509195  0.95017926]\n",
      "\n",
      "Test Accuracy: 0.7598103894834579\n",
      "Test Precision: [0.98245123 0.24285714]\n",
      "Test Recall: [0.75080145 0.85632513]\n",
      "Test F-Score: [0.85114631 0.37839889]\n",
      "-----------\n",
      "Training Neural Net\n",
      "Train Accuracy: 0.7685889570552147\n",
      "Train Precision: [0.70530857 0.88825825]\n",
      "Train Recall: [0.92269939 0.61447853]\n",
      "Train F-Score: [0.79948969 0.72642878]\n",
      "\n",
      "Test Accuracy: 0.8788056492205444\n",
      "Test Precision: [0.96098807 0.37153873]\n",
      "Test Recall: [0.90419962 0.60675444]\n",
      "Test F-Score: [0.93172934 0.46086957]\n",
      "-----------\n",
      "Training AdaBoost\n",
      "Train Accuracy: 0.7577914110429448\n",
      "Train Precision: [0.6912434  0.89537072]\n",
      "Train Recall: [0.93177914 0.58380368]\n",
      "Train F-Score: [0.79368729 0.70677362]\n",
      "\n",
      "Test Accuracy: 0.8968381957679714\n",
      "Test Precision: [0.96040592 0.42510288]\n",
      "Test Recall: [0.92535798 0.59129937]\n",
      "Test F-Score: [0.94255626 0.49461336]\n",
      "-----------\n",
      "Training Naive Bayes\n",
      "Train Accuracy: 0.7391411042944785\n",
      "Train Precision: [0.68501994 0.83801596]\n",
      "Train Recall: [0.88539877 0.59288344]\n",
      "Train F-Score: [0.7724256  0.69445243]\n",
      "\n",
      "Test Accuracy: 0.8597957288765088\n",
      "Test Precision: [0.9586686 0.3240276]\n",
      "Test Recall: [0.88485788 0.59129937]\n",
      "Test F-Score: [0.92028563 0.41864235]\n",
      "-----------\n"
     ]
    }
   ],
   "source": [
    "# Evaluate all models for \"icu\"\n",
    "# Form train test\n",
    "train, test = train_test_split(icu_res_known, shuffle=True, test_size=0.3, stratify=icu_res_known['icu'])\n",
    "# Check if the resutls are balanced and get obs, results which are balanced\n",
    "obs, results = checkBinaryBalance(train, features, 'icu', 0, 1)\n",
    "sampled_icu_scores = allModelEvaluation(names, classifiers, new_train[features], new_train['icu'], test[features], test['icu'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Hyper-parameter Optimization**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8879929629086645\n",
      "Precision: [0.94683861 0.36930456]\n",
      "Recall: [0.92973926 0.44075558]\n",
      "F-Score: [0.93821103 0.40187891]\n"
     ]
    }
   ],
   "source": [
    "# Based on the above results we can gauge that _____ is the best model. So let us try running hyperparemeter optimization on it\n",
    "# Hyperparameter tuning\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "calibrated_forest = CalibratedClassifierCV(\n",
    "   base_estimator=RandomForestClassifier())\n",
    "param_grid = {\n",
    "   'base_estimator__max_depth': [20, 40, 60, 80, 100, 120],\n",
    "    'base_estimator__n_estimators': [10, 50, 100, 150, 200]}\n",
    "search = GridSearchCV(calibrated_forest, param_grid, cv=5)\n",
    "search.fit(obs, results)\n",
    "dump(search, 'icu_hyperparameter_rf.joblib') \n",
    "print(\"Accuracy: {}\".format(search.score(test[features], test['icu'])))\n",
    "metrics = precision_recall_fscore_support(test['icu'], search.predict(test[features]))\n",
    "print(\"Precision: {}\\nRecall: {}\\nF-Score: {}\".format(metrics[0], metrics[1], metrics[2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Not Balanced returning sampled dataet.\n",
      "Best params for recall_score\n",
      "{'max_depth': 30, 'min_samples_split': 3, 'n_estimators': 200}\n",
      "\n",
      "Confusion matrix of Random Forest optimized for recall_score on the test data:\n",
      "     pred_neg  pred_pos\n",
      "neg     14305      4411\n",
      "pos       256      1491\n"
     ]
    }
   ],
   "source": [
    "# Hyper-parameter tuning based on custom scorer (accuracy, precision, or recall) and leveraging K-Fold startification for Grid \n",
    "# Search on Validation and Training Set\n",
    "# Source: https://towardsdatascience.com/fine-tuning-a-classifier-in-scikit-learn-66e048c21e65\n",
    "\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, StratifiedKFold\n",
    "from sklearn.metrics import roc_curve, precision_recall_curve, auc, make_scorer, recall_score, accuracy_score, precision_score, confusion_matrix\n",
    "\n",
    "obs, results = checkBinaryBalance(train, features, 'icu', 0, 1)\n",
    "clf = RandomForestClassifier()\n",
    "\n",
    "param_grid = {\n",
    "    'min_samples_split': [1, 3, 5, 10, 15, 20], \n",
    "    'n_estimators' : [20, 50, 100, 150, 200],\n",
    "    'max_depth': [3, 5, 15, 25, 30],\n",
    "}\n",
    "\n",
    "scorers = {\n",
    "    'precision_score': make_scorer(precision_score),\n",
    "    'recall_score': make_scorer(recall_score),\n",
    "    'accuracy_score': make_scorer(accuracy_score)\n",
    "}\n",
    "\n",
    "def grid_search_wrapper(refit_score='precision_score'):\n",
    "    \"\"\"\n",
    "    fits a GridSearchCV classifier using refit_score for optimization\n",
    "    prints classifier performance metrics\n",
    "    \"\"\"\n",
    "    skf = StratifiedKFold(n_splits=10)\n",
    "    grid_search = GridSearchCV(clf, param_grid, scoring=scorers, refit=refit_score,\n",
    "                           cv=skf, return_train_score=True, n_jobs=-1)\n",
    "    grid_search.fit(new_train[features], new_train['icu'])\n",
    "\n",
    "    # make the predictions\n",
    "    y_pred = grid_search.predict(test[features].values)\n",
    "\n",
    "    print('Best params for {}'.format(refit_score))\n",
    "    print(grid_search.best_params_)\n",
    "\n",
    "    # confusion matrix on the test data.\n",
    "    print('\\nConfusion matrix of Random Forest optimized for {} on the test data:'.format(refit_score))\n",
    "    print(pd.DataFrame(confusion_matrix(test['icu'], y_pred),\n",
    "                 columns=['pred_neg', 'pred_pos'], index=['neg', 'pos']))\n",
    "    return grid_search\n",
    "\n",
    "grid_search_clf = grid_search_wrapper(refit_score='recall_score')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8534630795649685\n",
      "Precision: [0.98241879 0.25262623]\n",
      "Recall: [0.7643193  0.85346308]\n",
      "F-Score: [0.85975298 0.38985488]\n"
     ]
    }
   ],
   "source": [
    "print(\"Accuracy: {}\".format(grid_search_clf.score(test[features], test['icu'])))\n",
    "metrics = precision_recall_fscore_support(test['icu'], grid_search_clf.predict(test[features]))\n",
    "print(\"Precision: {}\\nRecall: {}\\nF-Score: {}\".format(metrics[0], metrics[1], metrics[2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7740311782241118\n",
      "Precision: [0.98134991 0.25304721]\n",
      "Recall: [0.76752511 0.84373211]\n",
      "F-Score: [0.86136595 0.38932911]\n"
     ]
    }
   ],
   "source": [
    "# Train using recommended parameters\n",
    "temp_rf = RandomForestClassifier(n_estimators=100, min_samples_split=3, max_depth=30).fit(new_train[features], new_train['icu'])\n",
    "print(\"Accuracy: {}\".format(temp_rf.score(test[features], test['icu'])))\n",
    "metrics = precision_recall_fscore_support(test['icu'], temp_rf.predict(test[features]))\n",
    "print(\"Precision: {}\\nRecall: {}\\nF-Score: {}\".format(metrics[0], metrics[1], metrics[2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['recall_optimized_rf.joblib']"
      ]
     },
     "execution_count": 284,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Save Model\n",
    "dump(temp_rf, 'recall_optimized_rf.joblib')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Keras Fully-connected**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Not Balanced returning sampled dataet.\n"
     ]
    }
   ],
   "source": [
    "# Let us now train on a Dense Model in Keras\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras as keras\n",
    "from tensorflow.keras import datasets, layers, models\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "# Check if the resutls are balanced and get obs, results which are balanced\n",
    "obs, results = checkBinaryBalance(train, features, 'icu', 0, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_9\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_28 (Dense)             (None, 64)                1088      \n",
      "_________________________________________________________________\n",
      "dropout_6 (Dropout)          (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_29 (Dense)             (None, 32)                2080      \n",
      "_________________________________________________________________\n",
      "dropout_7 (Dropout)          (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_30 (Dense)             (None, 16)                528       \n",
      "_________________________________________________________________\n",
      "dense_31 (Dense)             (None, 1)                 17        \n",
      "=================================================================\n",
      "Total params: 3,713\n",
      "Trainable params: 3,713\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Build a Fully Connected Model\n",
    "model = models.Sequential()\n",
    "model.add(layers.Dense(64, activation='relu', input_dim=len(features)))\n",
    "model.add(layers.Dropout(0.2))\n",
    "model.add(layers.Dense(32, activation='relu'))\n",
    "model.add(layers.Dropout(0.2))\n",
    "model.add(layers.Dense(16, activation='relu'))\n",
    "model.add(layers.Dense(1, activation='sigmoid'))\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Checkpoints\n",
    "# Callback to Early Stop if Validation loss is not improving\n",
    "es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=10)\n",
    "# Callback to Keep storing best model\n",
    "mc = ModelCheckpoint(\"icu.h5\", monitor='val_accuracy', mode='max', verbose=1, save_best_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_obs, validation_obs, test_res, validation_res = train_test_split(test[features], test['icu'], test_size=0.5, shuffle=True, stratify=test['icu'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "1206/1229 [============================>.] - ETA: 0s - loss: 0.4923 - accuracy: 0.8003\n",
      "Epoch 00001: val_accuracy improved from -inf to 0.91067, saving model to icu_new.h5\n",
      "1229/1229 [==============================] - 3s 2ms/step - loss: 0.4922 - accuracy: 0.8003 - val_loss: 0.3595 - val_accuracy: 0.9107\n",
      "Epoch 2/100\n",
      "1216/1229 [============================>.] - ETA: 0s - loss: 0.4756 - accuracy: 0.8119\n",
      "Epoch 00002: val_accuracy improved from 0.91067 to 0.91077, saving model to icu_new.h5\n",
      "1229/1229 [==============================] - 2s 1ms/step - loss: 0.4751 - accuracy: 0.8122 - val_loss: 0.3310 - val_accuracy: 0.9108\n",
      "Epoch 3/100\n",
      "1222/1229 [============================>.] - ETA: 0s - loss: 0.4720 - accuracy: 0.8124\n",
      "Epoch 00003: val_accuracy improved from 0.91077 to 0.91087, saving model to icu_new.h5\n",
      "1229/1229 [==============================] - 1s 1ms/step - loss: 0.4720 - accuracy: 0.8124 - val_loss: 0.3509 - val_accuracy: 0.9109\n",
      "Epoch 4/100\n",
      "1223/1229 [============================>.] - ETA: 0s - loss: 0.4705 - accuracy: 0.8132\n",
      "Epoch 00004: val_accuracy did not improve from 0.91087\n",
      "1229/1229 [==============================] - 1s 1ms/step - loss: 0.4706 - accuracy: 0.8132 - val_loss: 0.3340 - val_accuracy: 0.9108\n",
      "Epoch 5/100\n",
      "1223/1229 [============================>.] - ETA: 0s - loss: 0.4694 - accuracy: 0.8127\n",
      "Epoch 00005: val_accuracy did not improve from 0.91087\n",
      "1229/1229 [==============================] - 1s 1ms/step - loss: 0.4697 - accuracy: 0.8126 - val_loss: 0.3360 - val_accuracy: 0.9098\n",
      "Epoch 6/100\n",
      "1220/1229 [============================>.] - ETA: 0s - loss: 0.4687 - accuracy: 0.8126\n",
      "Epoch 00006: val_accuracy did not improve from 0.91087\n",
      "1229/1229 [==============================] - 1s 1ms/step - loss: 0.4687 - accuracy: 0.8125 - val_loss: 0.3449 - val_accuracy: 0.9107\n",
      "Epoch 7/100\n",
      "1194/1229 [============================>.] - ETA: 0s - loss: 0.4670 - accuracy: 0.8126\n",
      "Epoch 00007: val_accuracy did not improve from 0.91087\n",
      "1229/1229 [==============================] - 1s 1ms/step - loss: 0.4665 - accuracy: 0.8129 - val_loss: 0.3231 - val_accuracy: 0.9108\n",
      "Epoch 8/100\n",
      "1216/1229 [============================>.] - ETA: 0s - loss: 0.4656 - accuracy: 0.8131\n",
      "Epoch 00008: val_accuracy improved from 0.91087 to 0.91106, saving model to icu_new.h5\n",
      "1229/1229 [==============================] - 1s 1ms/step - loss: 0.4660 - accuracy: 0.8128 - val_loss: 0.3389 - val_accuracy: 0.9111\n",
      "Epoch 9/100\n",
      "1219/1229 [============================>.] - ETA: 0s - loss: 0.4650 - accuracy: 0.8132\n",
      "Epoch 00009: val_accuracy did not improve from 0.91106\n",
      "1229/1229 [==============================] - 1s 1ms/step - loss: 0.4652 - accuracy: 0.8132 - val_loss: 0.3309 - val_accuracy: 0.9107\n",
      "Epoch 10/100\n",
      "1195/1229 [============================>.] - ETA: 0s - loss: 0.4646 - accuracy: 0.8128\n",
      "Epoch 00010: val_accuracy did not improve from 0.91106\n",
      "1229/1229 [==============================] - 1s 1ms/step - loss: 0.4641 - accuracy: 0.8133 - val_loss: 0.3419 - val_accuracy: 0.9105\n",
      "Epoch 11/100\n",
      "1203/1229 [============================>.] - ETA: 0s - loss: 0.4625 - accuracy: 0.8135\n",
      "Epoch 00011: val_accuracy did not improve from 0.91106\n",
      "1229/1229 [==============================] - 2s 1ms/step - loss: 0.4622 - accuracy: 0.8137 - val_loss: 0.3214 - val_accuracy: 0.9103\n",
      "Epoch 12/100\n",
      "1193/1229 [============================>.] - ETA: 0s - loss: 0.4632 - accuracy: 0.8132\n",
      "Epoch 00012: val_accuracy did not improve from 0.91106\n",
      "1229/1229 [==============================] - 1s 1ms/step - loss: 0.4622 - accuracy: 0.8138 - val_loss: 0.3155 - val_accuracy: 0.9096\n",
      "Epoch 13/100\n",
      "1180/1229 [===========================>..] - ETA: 0s - loss: 0.4605 - accuracy: 0.8133\n",
      "Epoch 00013: val_accuracy did not improve from 0.91106\n",
      "1229/1229 [==============================] - 1s 1ms/step - loss: 0.4597 - accuracy: 0.8140 - val_loss: 0.3251 - val_accuracy: 0.9109\n",
      "Epoch 14/100\n",
      "1202/1229 [============================>.] - ETA: 0s - loss: 0.4589 - accuracy: 0.8147\n",
      "Epoch 00014: val_accuracy did not improve from 0.91106\n",
      "1229/1229 [==============================] - 1s 1ms/step - loss: 0.4593 - accuracy: 0.8145 - val_loss: 0.3510 - val_accuracy: 0.9088\n",
      "Epoch 15/100\n",
      "1217/1229 [============================>.] - ETA: 0s - loss: 0.4595 - accuracy: 0.8139\n",
      "Epoch 00015: val_accuracy did not improve from 0.91106\n",
      "1229/1229 [==============================] - 1s 1ms/step - loss: 0.4592 - accuracy: 0.8141 - val_loss: 0.3280 - val_accuracy: 0.9106\n",
      "Epoch 16/100\n",
      "1229/1229 [==============================] - ETA: 0s - loss: 0.4586 - accuracy: 0.8145\n",
      "Epoch 00016: val_accuracy did not improve from 0.91106\n",
      "1229/1229 [==============================] - 1s 1ms/step - loss: 0.4586 - accuracy: 0.8145 - val_loss: 0.3283 - val_accuracy: 0.9104\n",
      "Epoch 17/100\n",
      "1209/1229 [============================>.] - ETA: 0s - loss: 0.4576 - accuracy: 0.8142\n",
      "Epoch 00017: val_accuracy did not improve from 0.91106\n",
      "1229/1229 [==============================] - 1s 1ms/step - loss: 0.4574 - accuracy: 0.8143 - val_loss: 0.3429 - val_accuracy: 0.9094\n",
      "Epoch 18/100\n",
      "1224/1229 [============================>.] - ETA: 0s - loss: 0.4571 - accuracy: 0.8146\n",
      "Epoch 00018: val_accuracy improved from 0.91106 to 0.91145, saving model to icu_new.h5\n",
      "1229/1229 [==============================] - 1s 1ms/step - loss: 0.4573 - accuracy: 0.8145 - val_loss: 0.3091 - val_accuracy: 0.9115\n",
      "Epoch 19/100\n",
      "1192/1229 [============================>.] - ETA: 0s - loss: 0.4564 - accuracy: 0.8146\n",
      "Epoch 00019: val_accuracy did not improve from 0.91145\n",
      "1229/1229 [==============================] - 1s 1ms/step - loss: 0.4563 - accuracy: 0.8146 - val_loss: 0.3380 - val_accuracy: 0.9096\n",
      "Epoch 20/100\n",
      "1210/1229 [============================>.] - ETA: 0s - loss: 0.4555 - accuracy: 0.8159\n",
      "Epoch 00020: val_accuracy did not improve from 0.91145\n",
      "1229/1229 [==============================] - 1s 1ms/step - loss: 0.4557 - accuracy: 0.8156 - val_loss: 0.3408 - val_accuracy: 0.9073\n",
      "Epoch 21/100\n",
      "1187/1229 [===========================>..] - ETA: 0s - loss: 0.4539 - accuracy: 0.8158\n",
      "Epoch 00021: val_accuracy did not improve from 0.91145\n",
      "1229/1229 [==============================] - 1s 1ms/step - loss: 0.4550 - accuracy: 0.8152 - val_loss: 0.3631 - val_accuracy: 0.9079\n",
      "Epoch 22/100\n",
      "1182/1229 [===========================>..] - ETA: 0s - loss: 0.4539 - accuracy: 0.8155\n",
      "Epoch 00022: val_accuracy did not improve from 0.91145\n",
      "1229/1229 [==============================] - 1s 1ms/step - loss: 0.4544 - accuracy: 0.8152 - val_loss: 0.3351 - val_accuracy: 0.9104\n",
      "Epoch 23/100\n",
      "1215/1229 [============================>.] - ETA: 0s - loss: 0.4541 - accuracy: 0.8154\n",
      "Epoch 00023: val_accuracy did not improve from 0.91145\n",
      "1229/1229 [==============================] - 2s 1ms/step - loss: 0.4539 - accuracy: 0.8156 - val_loss: 0.3435 - val_accuracy: 0.9076\n",
      "Epoch 24/100\n",
      "1229/1229 [==============================] - ETA: 0s - loss: 0.4535 - accuracy: 0.8154\n",
      "Epoch 00024: val_accuracy did not improve from 0.91145\n",
      "1229/1229 [==============================] - 2s 1ms/step - loss: 0.4535 - accuracy: 0.8154 - val_loss: 0.3444 - val_accuracy: 0.9086\n",
      "Epoch 25/100\n",
      "1194/1229 [============================>.] - ETA: 0s - loss: 0.4533 - accuracy: 0.8151\n",
      "Epoch 00025: val_accuracy did not improve from 0.91145\n",
      "1229/1229 [==============================] - 2s 1ms/step - loss: 0.4534 - accuracy: 0.8152 - val_loss: 0.3353 - val_accuracy: 0.9092\n",
      "Epoch 26/100\n",
      "1209/1229 [============================>.] - ETA: 0s - loss: 0.4538 - accuracy: 0.8155\n",
      "Epoch 00026: val_accuracy did not improve from 0.91145\n",
      "1229/1229 [==============================] - 2s 1ms/step - loss: 0.4533 - accuracy: 0.8157 - val_loss: 0.3313 - val_accuracy: 0.9079\n",
      "Epoch 27/100\n",
      "1192/1229 [============================>.] - ETA: 0s - loss: 0.4523 - accuracy: 0.8156\n",
      "Epoch 00027: val_accuracy did not improve from 0.91145\n",
      "1229/1229 [==============================] - 1s 935us/step - loss: 0.4515 - accuracy: 0.8161 - val_loss: 0.3354 - val_accuracy: 0.9077\n",
      "Epoch 28/100\n",
      "1201/1229 [============================>.] - ETA: 0s - loss: 0.4507 - accuracy: 0.8161\n",
      "Epoch 00028: val_accuracy did not improve from 0.91145\n",
      "1229/1229 [==============================] - 1s 969us/step - loss: 0.4512 - accuracy: 0.8156 - val_loss: 0.3472 - val_accuracy: 0.9082\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00028: early stopping\n"
     ]
    }
   ],
   "source": [
    "# Fit the model using training data with validation using a subset of the test data\n",
    "model_train_data = model.fit(obs, results, validation_data=(validation_obs, validation_res), epochs=100, batch_size=32, shuffle=True, callbacks=[es, mc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  1/640 [..............................] - ETA: 0s - loss: 0.2182 - accuracy: 0.9688WARNING:tensorflow:Callbacks method `on_test_batch_end` is slow compared to the batch time (batch time: 0.0000s vs `on_test_batch_end` time: 0.0010s). Check your callbacks.\n",
      "640/640 [==============================] - 0s 503us/step - loss: 0.3077 - accuracy: 0.9125\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.3076624870300293, 0.9125250577926636]"
      ]
     },
     "execution_count": 290,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load Best Model Weights and Evaluate Model on entire test set\n",
    "model.load_weights(\"icu.h5\") \n",
    "model.evaluate(test[features], test['icu'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "320/320 [==============================] - 0s 484us/step - loss: 0.3062 - accuracy: 0.9136\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.30622386932373047, 0.9135959148406982]"
      ]
     },
     "execution_count": 291,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Evaluate Model on subset of test set\n",
    "model.evaluate(test_obs, test_res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: [0.95637403 0.48879625]\n",
      "Recall: [0.94758495 0.53692044]\n",
      "F-Score: [0.95195921 0.51172941]\n"
     ]
    }
   ],
   "source": [
    "# Precision, Recall, F-score metrics for entire test set\n",
    "preds = model.predict(test[features])\n",
    "labels = []\n",
    "for pred in preds[:, 0]:\n",
    "    if pred > 0.5:\n",
    "        labels.append(1)\n",
    "    else:\n",
    "        labels.append(0)\n",
    "metrics = precision_recall_fscore_support(test['icu'], labels)\n",
    "print(\"Precision: {}\\nRecall: {}\\nF-Score: {}\".format(metrics[0], metrics[1], metrics[2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: [0.95795504 0.49438202]\n",
      "Recall: [0.94710408 0.55441008]\n",
      "F-Score: [0.95249866 0.52267819]\n"
     ]
    }
   ],
   "source": [
    "# Precision, Recall, F-score metrics for subset of test set\n",
    "preds = model.predict(test_obs)\n",
    "labels_test = []\n",
    "for pred in preds[:, 0]:\n",
    "    if pred > 0.5:\n",
    "        labels_test.append(1)\n",
    "    else:\n",
    "        labels_test.append(0)\n",
    "metrics = precision_recall_fscore_support(test_res, labels_test)\n",
    "print(\"Precision: {}\\nRecall: {}\\nF-Score: {}\".format(metrics[0], metrics[1], metrics[2]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lethality Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(220657, 21)"
      ]
     },
     "execution_count": 294,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Form dataset with only death status known\n",
    "lethal_res_known = covid_cases[covid_cases['lethal'] != -1]\n",
    "lethal_res_known.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Form train test\n",
    "train, test = train_test_split(lethal_res_known, shuffle=True, test_size=0.3, stratify=lethal_res_known['lethal'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Not Balanced returning sampled dataet.\n"
     ]
    }
   ],
   "source": [
    "# Check if the resutls are balanced and get obs, results which are balanced\n",
    "obs, results = checkBinaryBalance(train, features, 'lethal', 0, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Random Forest Evaluation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8442400072509744\n",
      "Precision: [0.97595838 0.43217961]\n",
      "Recall: [0.84318487 0.85176991]\n",
      "F-Score: [0.90472626 0.57341442]\n"
     ]
    }
   ],
   "source": [
    "# Evaluate Random Forest on training from the sampled dataset\n",
    "randomForestEvaluation(obs, results, test[features], test['lethal'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8953593764162059\n",
      "Precision: [0.91408211 0.63575118]\n",
      "Recall: [0.97206435 0.34795969]\n",
      "F-Score: [0.94218201 0.44975772]\n"
     ]
    }
   ],
   "source": [
    "# Evaluate Rnadom forest on training form the original dataset\n",
    "randomForestEvaluation(train[features], train['lethal'], test[features], test['lethal'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Not Balanced returning sampled dataet.\n",
      "Training Decision Tree\n",
      "Train Accuracy: 0.9521677082820728\n",
      "Train Precision: [0.95539702 0.94532289]\n",
      "Train Recall: [0.97370946 0.9090842 ]\n",
      "Train F-Score: [0.96446632 0.92684946]\n",
      "\n",
      "Test Accuracy: 0.8544366899302094\n",
      "Test Precision: [0.92988779 0.42404294]\n",
      "Test Recall: [0.90205298 0.51462635]\n",
      "Test F-Score: [0.91575892 0.46496391]\n",
      "-----------\n",
      "Training Random Forest\n",
      "Train Accuracy: 0.9508472352082411\n",
      "Train Precision: [0.96590389 0.92126444]\n",
      "Train Recall: [0.96016436 0.93221298]\n",
      "Train F-Score: [0.96302557 0.92670637]\n",
      "\n",
      "Test Accuracy: 0.8564458140729327\n",
      "Test Precision: [0.94187126 0.43924984]\n",
      "Test Recall: [0.89134029 0.6074238 ]\n",
      "Test F-Score: [0.91590935 0.50982617]\n",
      "-----------\n",
      "Training Neural Net\n",
      "Train Accuracy: 0.8503190459787084\n",
      "Train Precision: [0.91824141 0.74041228]\n",
      "Train Recall: [0.85127454 0.84840805]\n",
      "Train F-Score: [0.88349081 0.79073981]\n",
      "\n",
      "Test Accuracy: 0.8471403969908456\n",
      "Test Precision: [0.97194495 0.43563778]\n",
      "Test Recall: [0.85026351 0.82485251]\n",
      "Test F-Score: [0.90704148 0.5701542 ]\n",
      "-----------\n",
      "Training AdaBoost\n",
      "Train Accuracy: 0.8409199022357824\n",
      "Train Precision: [0.8848852  0.75580331]\n",
      "Train Recall: [0.8752399  0.77227991]\n",
      "Train F-Score: [0.88003612 0.76395278]\n",
      "\n",
      "Test Accuracy: 0.8582434514637904\n",
      "Test Precision: [0.96007712 0.45306859]\n",
      "Test Recall: [0.87475457 0.74041298]\n",
      "Test F-Score: [0.91543203 0.56215006]\n",
      "-----------\n",
      "Training Naive Bayes\n",
      "Train Accuracy: 0.8071453176516904\n",
      "Train Precision: [0.85700161 0.70879657]\n",
      "Train Recall: [0.85305841 0.71531913]\n",
      "Train F-Score: [0.85502546 0.71204291]\n",
      "\n",
      "Test Accuracy: 0.8363545726456992\n",
      "Test Precision: [0.95516663 0.40581127]\n",
      "Test Recall: [0.85348421 0.71411013]\n",
      "Test F-Score: [0.90146712 0.5175255 ]\n",
      "-----------\n"
     ]
    }
   ],
   "source": [
    "# Evaluate all models for \"lethal\"\n",
    "# Form train test\n",
    "train, test = train_test_split(lethal_res_known, shuffle=True, test_size=0.3, stratify=lethal_res_known['lethal'])\n",
    "# Check if the resutls are balanced and get obs, results which are balanced\n",
    "obs, results = checkBinaryBalance(train, features, 'lethal', 0, 1)\n",
    "sampled_lethal_scores = allModelEvaluation(names, classifiers, obs, results, test[features], test['lethal'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8627602042357775\n",
      "Precision: [0.95673014 0.4622844 ]\n",
      "Recall: [0.88348662 0.71484759]\n",
      "F-Score: [0.91865078 0.56147126]\n"
     ]
    }
   ],
   "source": [
    "# Based on the above results we can gauge that _____ is the best model. So let us try running hyperparemeter optimization on it\n",
    "# Hyperparameter tuning\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "calibrated_forest = CalibratedClassifierCV(\n",
    "   base_estimator=RandomForestClassifier())\n",
    "param_grid = {\n",
    "   'base_estimator__max_depth': [20, 40, 60, 80, 100, 120],\n",
    "    'base_estimator__n_estimators': [10, 50, 100, 150, 200]}\n",
    "search = GridSearchCV(calibrated_forest, param_grid, cv=5)\n",
    "search.fit(obs, results)\n",
    "dump(search, 'lethal_hyperparameter_rf.joblib') \n",
    "print(\"Accuracy: {}\".format(search.score(test[features], test['lethal'])))\n",
    "metrics = precision_recall_fscore_support(test['lethal'], search.predict(test[features]))\n",
    "print(\"Precision: {}\\nRecall: {}\\nF-Score: {}\".format(metrics[0], metrics[1], metrics[2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_10\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_32 (Dense)             (None, 64)                1152      \n",
      "_________________________________________________________________\n",
      "dropout_8 (Dropout)          (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_33 (Dense)             (None, 32)                2080      \n",
      "_________________________________________________________________\n",
      "dropout_9 (Dropout)          (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_34 (Dense)             (None, 16)                528       \n",
      "_________________________________________________________________\n",
      "dense_35 (Dense)             (None, 1)                 17        \n",
      "=================================================================\n",
      "Total params: 3,777\n",
      "Trainable params: 3,777\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Build a Fully Connected Model\n",
    "model = models.Sequential()\n",
    "model.add(layers.Dense(64, activation='relu', input_dim=17))\n",
    "model.add(layers.Dropout(0.2))\n",
    "model.add(layers.Dense(32, activation='relu'))\n",
    "model.add(layers.Dropout(0.2))\n",
    "model.add(layers.Dense(16, activation='relu'))\n",
    "model.add(layers.Dense(1, activation='sigmoid'))\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Checkpoints\n",
    "# Callback to Early Stop if Validation loss is not improving\n",
    "es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=10)\n",
    "# Callback to Keep storing best model\n",
    "mc = ModelCheckpoint(\"lethal.h5\", monitor='val_accuracy', mode='max', verbose=1, save_best_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_obs, validation_obs, test_res, validation_res = train_test_split(test[features], test['lethal'], test_size=0.5, shuffle=True, stratify=test['lethal'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "in user code:\n\n    C:\\Users\\soumi\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\keras\\engine\\training.py:806 train_function  *\n        return step_function(self, iterator)\n    C:\\Users\\soumi\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\keras\\engine\\training.py:796 step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    C:\\Users\\soumi\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\distribute\\distribute_lib.py:1211 run\n        return self._extended.call_for_each_replica(fn, args=args, kwargs=kwargs)\n    C:\\Users\\soumi\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\distribute\\distribute_lib.py:2585 call_for_each_replica\n        return self._call_for_each_replica(fn, args, kwargs)\n    C:\\Users\\soumi\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\distribute\\distribute_lib.py:2945 _call_for_each_replica\n        return fn(*args, **kwargs)\n    C:\\Users\\soumi\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\keras\\engine\\training.py:789 run_step  **\n        outputs = model.train_step(data)\n    C:\\Users\\soumi\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\keras\\engine\\training.py:747 train_step\n        y_pred = self(x, training=True)\n    C:\\Users\\soumi\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\keras\\engine\\base_layer.py:975 __call__\n        input_spec.assert_input_compatibility(self.input_spec, inputs,\n    C:\\Users\\soumi\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\keras\\engine\\input_spec.py:212 assert_input_compatibility\n        raise ValueError(\n\n    ValueError: Input 0 of layer sequential_10 is incompatible with the layer: expected axis -1 of input shape to have value 17 but received input with shape [None, 16]\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-303-249cd2c15c34>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# Fit the model using training data with validation using a subset of the test data\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mmodel_train_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresults\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalidation_obs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalidation_res\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m32\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mes\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmc\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    106\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_method_wrapper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    107\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_in_multi_worker_mode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 108\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    109\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    110\u001b[0m     \u001b[1;31m# Running inside `run_distribute_coordinator` already.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1096\u001b[0m                 batch_size=batch_size):\n\u001b[0;32m   1097\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1098\u001b[1;33m               \u001b[0mtmp_logs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1099\u001b[0m               \u001b[1;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1100\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    778\u001b[0m       \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    779\u001b[0m         \u001b[0mcompiler\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"nonXla\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 780\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    781\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    782\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    821\u001b[0m       \u001b[1;31m# This is the first call of __call__, so we have to initialize.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    822\u001b[0m       \u001b[0minitializers\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 823\u001b[1;33m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_initialize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0madd_initializers_to\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minitializers\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    824\u001b[0m     \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    825\u001b[0m       \u001b[1;31m# At this point we know that the initialization is complete (or less\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m_initialize\u001b[1;34m(self, args, kwds, add_initializers_to)\u001b[0m\n\u001b[0;32m    694\u001b[0m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_graph_deleter\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mFunctionDeleter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_lifted_initializer_graph\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    695\u001b[0m     self._concrete_stateful_fn = (\n\u001b[1;32m--> 696\u001b[1;33m         self._stateful_fn._get_concrete_function_internal_garbage_collected(  # pylint: disable=protected-access\n\u001b[0m\u001b[0;32m    697\u001b[0m             *args, **kwds))\n\u001b[0;32m    698\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_get_concrete_function_internal_garbage_collected\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   2853\u001b[0m       \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2854\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2855\u001b[1;33m       \u001b[0mgraph_function\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2856\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2857\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_maybe_define_function\u001b[1;34m(self, args, kwargs)\u001b[0m\n\u001b[0;32m   3211\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3212\u001b[0m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_function_cache\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmissed\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcall_context_key\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3213\u001b[1;33m       \u001b[0mgraph_function\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_create_graph_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3214\u001b[0m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_function_cache\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprimary\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mcache_key\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3215\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_create_graph_function\u001b[1;34m(self, args, kwargs, override_flat_arg_shapes)\u001b[0m\n\u001b[0;32m   3063\u001b[0m     \u001b[0marg_names\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbase_arg_names\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mmissing_arg_names\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3064\u001b[0m     graph_function = ConcreteFunction(\n\u001b[1;32m-> 3065\u001b[1;33m         func_graph_module.func_graph_from_py_func(\n\u001b[0m\u001b[0;32m   3066\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_name\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3067\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_python_function\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\framework\\func_graph.py\u001b[0m in \u001b[0;36mfunc_graph_from_py_func\u001b[1;34m(name, python_func, args, kwargs, signature, func_graph, autograph, autograph_options, add_control_dependencies, arg_names, op_return_value, collections, capture_by_value, override_flat_arg_shapes)\u001b[0m\n\u001b[0;32m    984\u001b[0m         \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moriginal_func\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_decorator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munwrap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpython_func\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    985\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 986\u001b[1;33m       \u001b[0mfunc_outputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpython_func\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mfunc_args\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mfunc_kwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    987\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    988\u001b[0m       \u001b[1;31m# invariant: `func_outputs` contains only Tensors, CompositeTensors,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36mwrapped_fn\u001b[1;34m(*args, **kwds)\u001b[0m\n\u001b[0;32m    598\u001b[0m         \u001b[1;31m# __wrapped__ allows AutoGraph to swap in a converted function. We give\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    599\u001b[0m         \u001b[1;31m# the function a weak reference to itself to avoid a reference cycle.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 600\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mweak_wrapped_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__wrapped__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    601\u001b[0m     \u001b[0mweak_wrapped_fn\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mweakref\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mref\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mwrapped_fn\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    602\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\framework\\func_graph.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    971\u001b[0m           \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# pylint:disable=broad-except\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    972\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"ag_error_metadata\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 973\u001b[1;33m               \u001b[1;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mag_error_metadata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_exception\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    974\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    975\u001b[0m               \u001b[1;32mraise\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: in user code:\n\n    C:\\Users\\soumi\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\keras\\engine\\training.py:806 train_function  *\n        return step_function(self, iterator)\n    C:\\Users\\soumi\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\keras\\engine\\training.py:796 step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    C:\\Users\\soumi\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\distribute\\distribute_lib.py:1211 run\n        return self._extended.call_for_each_replica(fn, args=args, kwargs=kwargs)\n    C:\\Users\\soumi\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\distribute\\distribute_lib.py:2585 call_for_each_replica\n        return self._call_for_each_replica(fn, args, kwargs)\n    C:\\Users\\soumi\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\distribute\\distribute_lib.py:2945 _call_for_each_replica\n        return fn(*args, **kwargs)\n    C:\\Users\\soumi\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\keras\\engine\\training.py:789 run_step  **\n        outputs = model.train_step(data)\n    C:\\Users\\soumi\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\keras\\engine\\training.py:747 train_step\n        y_pred = self(x, training=True)\n    C:\\Users\\soumi\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\keras\\engine\\base_layer.py:975 __call__\n        input_spec.assert_input_compatibility(self.input_spec, inputs,\n    C:\\Users\\soumi\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\keras\\engine\\input_spec.py:212 assert_input_compatibility\n        raise ValueError(\n\n    ValueError: Input 0 of layer sequential_10 is incompatible with the layer: expected axis -1 of input shape to have value 17 but received input with shape [None, 16]\n"
     ]
    }
   ],
   "source": [
    "# Fit the model using training data with validation using a subset of the test data\n",
    "model_train_data = model.fit(obs, results, validation_data=(validation_obs, validation_res), epochs=100, batch_size=32, shuffle=True, callbacks=[es, mc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Best Model Weights and Evaluate Model on entire test set\n",
    "model.load_weights(\"lethal.h5\") \n",
    "model.evaluate(test[features], test[\"lethal\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate Model on subset of test set\n",
    "model.evaluate(test_obs, test_res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Precision, Recall, F-score metrics for entire test set\n",
    "preds = model.predict(test[features])\n",
    "labels = []\n",
    "for pred in preds[:, 0]:\n",
    "    if pred > 0.5:\n",
    "        labels.append(1)\n",
    "    else:\n",
    "        labels.append(0)\n",
    "metrics = precision_recall_fscore_support(test[\"lethal\"], labels)\n",
    "print(\"Precision: {}\\nRecall: {}\\nF-Score: {}\".format(metrics[0], metrics[1], metrics[2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Precision, Recall, F-score metrics for subset of test set\n",
    "preds = model.predict(test_obs)\n",
    "labels_test = []\n",
    "for pred in preds[:, 0]:\n",
    "    if pred > 0.5:\n",
    "        labels_test.append(1)\n",
    "    else:\n",
    "        labels_test.append(0)\n",
    "metrics = precision_recall_fscore_support(test_res, labels_test)\n",
    "print(\"Precision: {}\\nRecall: {}\\nF-Score: {}\".format(metrics[0], metrics[1], metrics[2]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
